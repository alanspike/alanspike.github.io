<!DOCTYPE html
    PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html>

<head>
    <link rel="shortcut icon" type="image/x-icon" href="images/head.png" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />

    <title>Jian Ren</title>

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css">
    <link href='http://fonts.googleapis.com/css?family=Lato:400,700' rel='stylesheet' type='text/css'>
    <link href="css/style.css" rel="stylesheet" type="text/css" />

    <script type="text/javascript">

        var _gaq = _gaq || [];
        _gaq.push(['_setAccount', 'UA-40545479-1']);
        _gaq.push(['_trackPageview']);

        (function () {
            var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
            ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
            var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
        })();

    </script>


</head>

<body>
    <div class="container">
        <table width="900" border="0" align="center" cellpadding="20">
            <table width="90%" align="center" border="0" cellpadding="10">
                <tr>
                    <td width="100%" valign="top">
                        <p align="center">&nbsp;</p>

                        <p align="center">
                            <font size="6px">Jian Ren</font><br>
                        </p>

                        <p>
                            I lead a research team at Snap Inc. Our team works on <strong>Efficient AI</strong> and <strong>Generative AI</strong>, building image, video, and 3D rendering models. <br>

                            <!-- <strong>We are currently hiring Research Scientists, please reach out if you are interested.</strong> -->
                            <!-- Before joining Snap Inc.,
                            I got a Ph.D. at <a href="https://www.rutgers.edu/">Rutgers
                                University</a> under the supervision of <a
                                href="https://gemini.cinj.rutgers.edu/rutgers-people/david-j-foran-phd/">
                                David J. Foran</a>
                            and <a href="http://manishparashar.org/"> Manish Parashar</a> in 2019,
                            and a B.S. from <a href="http://en.ustc.edu.cn/">USTC</a>
                            in 2014. -->

                        <p> <a href="mailto:renjianustc@gmail.com" target="_blank"><img src="icons/email.png"
                                    height="24"></a> &nbsp&nbsp
                            <a href="https://www.linkedin.com/in/renjian0905" target="_blank"><img
                                    src="icons/linkedin.png" height="24"></a> &nbsp&nbsp
                            <a href="https://scholar.google.com/citations?user=vDALiU4AAAAJ&hl=en&authuser=1"
                                target="_blank"><img src="icons/google_scholar.png" height="24"></a> &nbsp&nbsp
                            <a href="https://github.com/alanspike/" target="_blank"><img src="icons/github.png"
                                    height="24"></a> &nbsp&nbsp
                            <a href="https://twitter.com/JianRen_" target="_blank"><img src="icons/twitter.png"
                                    height="24"></a>
                        </p>
                    </td>
                    <!-- <td width="50%" height="50%">
                        <div class="instructorphoto">
                            <img src="images/head.png">
                        </div>
                    </td> -->
                </tr>
            </table>
        </table>
    </div>
    <br>

    <div class="container">
        <h2> 2024 </h2>
        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/weight_generator.jpg" alt="game" width="190" 
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://yifanfanfanfan.github.io/denoised-weights/"><b>
                        Efficient Training with Denoised Neural Weights
                            </b></a><br>
                            Yifan Gong, Zheng Zhan, Yanyu Li, Yerlan Idelbayev, Andrey Zharkov,<br> Kfir Aberman, Sergey Tulyakov, Yanzhi Wang, 
                            <strong>Jian Ren</strong>
                        <br>
                        <em>ECCV</em>, 2024<br>
                    <div class="paper">
                        <a href="https://yifanfanfanfan.github.io/denoised-weights/">Project</a> /
                        <a href="https://arxiv.org/abs/2407.11966">arXiv</a> /
                        <a href="https://arxiv.org/pdf/2407.11966">PDF</a>
                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/e2gan.jpeg" alt="game" width="190" 
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://yifanfanfanfan.github.io/e2gan/"><b>
                        E<sup>2</sup>GAN: Efficient Training of Efficient GANs for Image-to-Image Translation
                            </b></a><br>
                            Yifan Gong, Zheng Zhan, Qing Jin, Yanyu Li, Yerlan Idelbayev, Xian Liu, <br>Andrey Zharkov, Kfir Aberman, Sergey Tulyakov, Yanzhi Wang, 
                            <strong>Jian Ren</strong>
                        <br>
                        <em>ICML</em>, 2024<br>
                    <div class="paper">
                        <a href="https://yifanfanfanfan.github.io/e2gan/">Project</a> /
                        <a href="https://arxiv.org/abs/2401.06127">arXiv</a> /
                        <a href="https://arxiv.org/pdf/2401.06127">PDF</a>
                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/textcraftor.png" alt="game" width="190" 
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://arxiv.org/abs/2403.18978"><b>
                        TextCraftor: Your Text Encoder Can be Image Quality Controller
                            </b></a><br>
                            Yanyu Li, Xian Liu, Anil Kag, Ju Hu, Yerlan Idelbayev, Dhritiman Sagar, Yanzhi Wang, <br>Sergey Tulyakov, <strong>Jian Ren</strong>
                        <br>
                        <em>CVPR</em>, 2024<br>
                    <div class="paper">
                        <a href="https://arxiv.org/abs/2403.18978">arXiv</a> /
                        <a href="https://arxiv.org/pdf/2403.18978">PDF</a>
                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/snapvideo.gif" alt="game" width="190" 
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://snap-research.github.io/snapvideo/"><b>
                        Snap Video: Scaled Spatiotemporal Transformers for Text-to-Video Synthesis
                            </b></a><br>
                            Willi Menapace, Aliaksandr Siarohin, Ivan Skorokhodov, Ekaterina Deyneka, <br>Tsai-Shien Chen, Anil Kag, Yuwei Fang, Aleksei Stoliar, Elisa Ricci, <strong>Jian Ren</strong>, Sergey Tulyakov
                        <br>
                        <em>CVPR</em>, 2024 <strong>(Highlight)</strong><br>
                    <div class="paper">
                        <a href="https://snap-research.github.io/snapvideo/">Project</a> /
                        <a href="https://arxiv.org/abs/2402.14797">arXiv</a> /
                        <a href="https://arxiv.org/pdf/2402.14797">PDF</a>
                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/panda.png" alt="game" width="190" 
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://snap-research.github.io/Panda-70M/"><b>
                        Panda-70M: Captioning 70M Videos with Multiple Cross-Modality Teachers
                            </b></a><br>
                            Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, <br>Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, 
                            <strong>Jian Ren</strong>, Ming-Hsuan Yang, Sergey Tulyakov
                        <br>
                        <em>CVPR</em>, 2024<br>
                    <div class="paper">
                        <a href="https://snap-research.github.io/Panda-70M/">Project</a> /
                        <a href="https://github.com/snap-research/Panda-70M">code</a> /
                        <a href="https://arxiv.org/abs/2402.19479">arXiv</a> /
                        <a href="https://arxiv.org/pdf/2402.19479">PDF</a>
                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/spad.gif" alt="game" width="190" 
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://yashkant.github.io/spad/"><b>
                        SPAD: Spatially Aware Multiview Diffusers
                            </b></a><br>
                            Yash Kant, Ziyi Wu, Michael Vasilkovsky, Guocheng Qian, <strong>Jian Ren</strong>, Riza Alp Guler, <br> 
                            Bernard Ghanem, Sergey Tulyakov, Igor Gilitschenski, Aliaksandr Siarohin
                        <br>
                        <em>CVPR</em>, 2024<br>
                    <div class="paper">
                        <a href="https://yashkant.github.io/spad/">Project</a> /
                        <a href="https://github.com/yashkant/spad">code</a> /
                        <a href="https://arxiv.org/abs/2402.05235">arXiv</a> /
                        <a href="https://arxiv.org/pdf/2402.05235">PDF</a>
                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/hyperhuman.gif" alt="game" width="190" 
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://snap-research.github.io/HyperHuman/"><b>
                        HyperHuman: Hyper-Realistic Human Generation with Latent Structural Diffusion
                            </b></a><br>
                        Xian Liu, <strong>Jian Ren</strong>, Aliaksandr Siarohin, Ivan Skorokhodov, Yanyu Li,<br> 
                        Dahua Lin, Xihui Liu, Ziwei Liu, Sergey Tulyakov
                        <br>
                        <em>ICLR</em>, 2024<br>
                    <div class="paper">
                        <a href="https://snap-research.github.io/HyperHuman/">Project</a> /
                        <a href="https://openreview.net/forum?id=duyA42HlCK/">OpenReview</a> /
                        <a href="https://arxiv.org/abs/2310.08579">arXiv</a> /
                        <a href="https://arxiv.org/pdf/2310.08579.pdf">PDF</a>
                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/magic123.gif" alt="game" width="190" 
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://guochengqian.github.io/project/magic123/"><b>
                        Magic123: One Image to High-Quality 3D Object Generation Using Both 2D and 3D Diffusion Priors
                            </b></a><br>
                            Guocheng Qian, Jinjie Mai, Abdullah Hamdi, <strong>Jian Ren</strong>, Aliaksandr Siarohin, Bing Li, <br>
                            Hsin-Ying Lee, Ivan Skorokhodov, Peter Wonka, Sergey Tulyakov, Bernard Ghanem<br>
                        <em>ICLR</em>, 2024<br>
                    <div class="paper">
                        <a href="https://guochengqian.github.io/project/magic123/">Project</a> /
                        <a href="https://openreview.net/forum?id=0jHkUDyEO9/">OpenReview</a> /
                        <a href="https://arxiv.org/abs/2306.17843">arXiv</a> /
                        <a href="https://arxiv.org/pdf/2306.17843.pdf">PDF</a> /
                        <a href="https://github.com/guochengqian/Magic123">code</a>
                    </div>
                </td>
                </td>
            </table>
        </div>
        <!-- <hr> -->
    </div>
    <br>

    <div class="container">
        <h2> 2023 </h2>
        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/snapinfocusfast_demo_short.gif" alt="game" width="110" 
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://snap-research.github.io/SnapFusion/"><b>
                        SnapFusion: Text-to-Image Diffusion Model on Mobile Devices within Two Seconds
                            </b></a><br>
                        Yanyu Li, Huan Wang, Qing Jin, Ju Hu, Pavlo Chemerys, Yun Fu, Yanzhi Wang, <br>Sergey Tulyakov, <strong>Jian Ren</strong><br>
                        <em>NeurIPs</em>, 2023<br>
                    <div class="paper">
                        <a href="https://snap-research.github.io/SnapFusion/">Project</a> /
                        <a href="https://arxiv.org/abs/2306.00980">arXiv</a> /
                        <a href="https://arxiv.org/pdf/2306.00980.pdf">PDF</a>
                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/lightspeed.png" alt="game" width="190" 
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://lightspeed-r2l.github.io"><b>LightSpeed: Light and Fast Neural Light Fields on Mobile Devices
                            </b></a><br>
                            Aarush Gupta, Junli Cao, Chaoyang Wang, Ju Hu, Sergey Tulyakov, <br>
                        <strong>Jian Ren</strong>, László A Jeni <br>
                        <em>NeurIPs</em>, 2023
                    <div class="paper">
                        <a href="https://lightspeed-r2l.github.io/">Project</a> /
                        <a href="https://arxiv.org/abs/2310.16832">arXiv</a> /
                        <a href="https://arxiv.org/pdf/2310.16832.pdf">PDF</a>
                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/invs_video.gif" alt="game" width="190" 
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://yashkant.github.io/invs/"><b>iNVS: Repurposing Diffusion Inpainters for Novel View Synthesis
                            </b></a><br>
                            Yash Kant, Aliaksandr Siarohin, Michael Vasilkovsky, Riza Alp Guler, <br>
                            <strong>Jian Ren</strong>, Sergey Tulyakov, Igor Gilitschenski<br>
                    <em>SIGGRAPH Asia</em>, 2023
                    <div class="paper">
                        <a href="https://yashkant.github.io/invs/">Project</a> /
                        <a href="https://arxiv.org/abs/2310.16167">arXiv</a> /
                        <a href="https://arxiv.org/pdf/2310.16167.pdf">PDF</a> 
                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/eformerv2.png" alt="game" width="190" height="146"
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://github.com/snap-research/EfficientFormer/"><b>Rethinking Vision Transformers for
                                MobileNet Size and Speed
                            </b></a><br>
                        Yanyu Li, Ju Hu, Yang Wen, Georgios Evangelidis, Kamyar Salahi, Yanzhi
                        Wang, <br> Sergey Tulyakov,
                        <strong>Jian Ren</strong> <br>
                        <em>ICCV</em>, 2023
                    <div class="paper">
                        <a href="https://github.com/snap-research/EfficientFormer">code</a> /
                        <a href="https://arxiv.org/abs/2212.08059">arXiv</a> /
                        <a href="https://arxiv.org/pdf/2212.08059.pdf">PDF</a>

                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/comcat.jpg" alt="game" width="190" height="92"
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://arxiv.org/abs/2305.17235"><b>COMCAT: Towards Efficient
                                Compression and Customization of
                                Attention-Based Vision Models
                            </b></a><br>
                        Jinqi Xiao, Miao Yin, Yu Gong, Xiao Zang, <strong>Jian Ren</strong>, Bo Yuan
                        <br>
                        <em>ICML</em>, 2023
                    <div class="paper">
                        <a href="https://github.com/jinqixiao/ComCAT">code</a> /
                        <a href="https://arxiv.org/abs/2305.17235">arXiv</a> /
                        <a href="https://arxiv.org/pdf/2305.17235.pdf">PDF</a>

                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/mobile-r2l.gif" alt="game" width="190" height="101"
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://snap-research.github.io/MobileR2L/"><b>
                                Real-Time Neural Light Field on Mobile Devices
                            </b></a><br>
                        Junli Cao, Huan Wang, Pavlo Chemerys, Vladislav Shakhrai, Ju Hu, Yun Fu, <br>Denys Makoviichuk,
                        Sergey
                        Tulyakov, <strong>Jian Ren</strong>
                        <br>
                        <em>CVPR</em>, 2023<br>
                    <div class="paper">
                        <a href="https://snap-research.github.io/MobileR2L/">Project</a> /
                        <a href="https://arxiv.org/abs/2212.08057">arXiv</a> /
                        <a href="https://github.com/snap-research/MobileR2L">code</a> /
                        <a href="https://arxiv.org/pdf/2212.08057.pdf">PDF</a>
                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/SINE.png" alt="game" width="190" height="170"
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://zhang-zx.github.io/SINE/"><b>
                                SINE: SINgle Image Editing with Text-to-Image Diffusion Models
                            </b></a><br>
                        Zhixing Zhang, Ligong Han, Arnab Ghosh, Dimitris N. Metaxas, <strong>Jian Ren</strong>
                        <br>
                        <em>CVPR</em>, 2023<br>
                    <div class="paper">
                        <a href="https://zhang-zx.github.io/SINE/">Project</a> /
                        <a href="https://arxiv.org/abs/2212.04489">arXiv</a> /
                        <a href="https://github.com/zhang-zx/SINE">code</a> /
                        <a href="https://arxiv.org/pdf/2212.04489.pdf">PDF</a>
                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>


        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/uva.gif" alt="game" width="190" height="160"
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://snap-research.github.io/unsupervised-volumetric-animation/"><b>Unsupervised
                                Volumetric
                                Animation

                            </b></a><br>
                        Aliaksandr Siarohin, Willi Menapace, Ivan Skorokhodov, <strong>Jian Ren</strong>, Hsin-Ying Lee,
                        <br> Menglei Chai, Kyle Olszewski, Sergey Tulyakov
                        <br>
                        <em>CVPR</em>, 2023<br>
                    <div class="paper">
                        <a href="https://snap-research.github.io/unsupervised-volumetric-animation/">Project</a> /
                        <a href="https://arxiv.org/abs/2301.11326">arXiv</a> /
                        <a href="https://github.com/snap-research/unsupervised-volumetric-animation">code</a> /
                        <a href="https://arxiv.org/pdf/2301.11326.pdf">PDF</a>
                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/make-a-story.png" alt="game" width="190" height="90"
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://yashkant.github.io/invertible-neural-skinning/"><b>Make-A-Story: Visual Memory
                                Conditioned Consistent Story Generation
                            </b></a><br>
                        Tanzila Rahman, Hsin-Ying Lee, <strong>Jian Ren</strong>, Sergey Tulyakov, Shweta Mahajan,<br>
                        Leonid Sigal
                        <br>
                        <em>CVPR</em>, 2023<br>
                    <div class="paper">
                        <a href="https://arxiv.org/abs/2211.13319">arXiv</a> /
                        <a href="https://arxiv.org/pdf/2211.13319.pdf">PDF</a>
                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/ins.gif" alt="game" width="190" height="107"
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://yashkant.github.io/invertible-neural-skinning/"><b>Invertible Neural Skinning
                            </b></a><br>
                        Yash Kant, Aliaksandr Siarohin, Riza Alp Guler, Menglei Chai, <strong>Jian Ren</strong>,
                        <br>Sergey
                        Tulyakov, Igor
                        Gilitschenski
                        <br>
                        <em>CVPR</em>, 2023<br>
                    <div class="paper">
                        <a href="https://yashkant.github.io/invertible-neural-skinning">Project</a> /
                        <a href="https://arxiv.org/abs/2302.09227">arXiv</a> /
                        <a href="https://github.com/yashkant/invertible-neural-skinning">code</a> /
                        <a href="https://www.youtube.com/watch?v=L7MrPzhqPWQ">video</a> /
                        <a href="https://arxiv.org/pdf/2302.09227.pdf">PDF</a>
                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/imagenet.gif" alt="game" width="190" height="190"
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://snap-research.github.io/3dgp/"><b>3D Generation on ImageNet
                            </b></a><br>
                        Ivan Skorokhodov, Aliaksandr Siarohin, Yinghao Xu, <strong>Jian Ren</strong>, Hsin-Ying Lee,<br>
                        Peter Wonka, Sergey Tulyakov
                        <br>
                        <em>ICLR</em>, 2023 <strong>(Oral)</strong><br>
                    <div class="paper">
                        <a href="https://snap-research.github.io/3dgp/">Project</a> /
                        <a href="https://openreview.net/forum?id=U2WjB9xxZ9q">OpenReview</a> /
                        <a href="https://arxiv.org/abs/2303.01416">arXiv</a> /
                        <a href="https://github.com/snap-research/3dgp">code</a> /
                        <a href="https://arxiv.org/pdf/2303.01416.pdf">PDF</a>
                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/cdcd.png" alt="game" width="190" height="153"
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://l-yezhu.github.io/CDCD/"><b>Discrete Contrastive Diffusion for
                                Cross-Modal and Conditional Generation
                            </b></a><br>
                        Ye Zhu, Yu Wu, Kyle Olszewski, <strong>Jian Ren</strong>, Sergey Tulyakov, Yan Yan <br>
                        <em>ICLR</em>, 2023
                    <div class="paper">
                        <a href="https://l-yezhu.github.io/CDCD/">Project</a> /
                        <a href="https://openreview.net/forum?id=1-MBdJssZ-S">OpenReview</a> /
                        <a href="https://arxiv.org/abs/2206.07771">arXiv</a> /
                        <a href="https://github.com/L-YeZhu/CDCD/">code</a> /
                        <a href="https://arxiv.org/pdf/2206.07771.pdf">PDF</a>
                    </div>
                </td>
                </td>
            </table>
        </div>
        <!-- <hr> -->


    </div>
    <br>

    <div class="container">
        <h2> 2022 </h2>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/efficientformer.png" alt="game" width="190"
                        height="148" style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://github.com/snap-research/EfficientFormer/"><b>EfficientFormer: Vision
                                Transformers at
                                MobileNet Speed
                            </b></a><br>
                        Yanyu Li, Geng Yuan, Yang Wen, Ju Hu, Georgios Evangelidis, Sergey Tulyakov, <br> Yanzhi
                        Wang,
                        <strong>Jian Ren</strong> <br>
                        <em>NeurIPs</em>, 2022
                    <div class="paper">
                        <a href="https://github.com/snap-research/EfficientFormer">code</a> /
                        <a href="https://arxiv.org/abs/2206.01191">arXiv</a> /
                        <a href="https://arxiv.org/pdf/2206.01191.pdf">PDF</a>

                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/freeze_train.png" alt="game" width="190" height="109"
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://github.com/snap-research/SpFDE"><b>Layer Freezing & Data Sieving: Missing
                                Pieces
                                of a
                                Generic Framework for Sparse Training
                            </b></a><br>
                        Geng Yuan, Yanyu Li, Sheng Li, Zhenglun Kong, Sergey Tulyakov, Xulong Tang, <br> Yanzhi
                        Wang,
                        <strong>Jian Ren</strong> <br>
                        <em>NeurIPs</em>, 2022
                    <div class="paper">
                        <a href="https://github.com/snap-research/SpFDE">code</a> /
                        <a href="https://arxiv.org/abs/2209.11204">arXiv</a> /
                        <a href="https://arxiv.org/pdf/2209.11204.pdf">PDF</a>

                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/r2l.gif" alt="game" width="190" height="95"
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://snap-research.github.io/R2L/"><b>R2L: Distilling Neural Radiance Field to
                                Neural Light Field for Efficient Novel View Synthesis</b></a><br>
                        Huan Wang, <strong>Jian Ren</strong>, Zeng Huang, Kyle Olszewski,
                        Menglei Chai, Yun Fu, Sergey Tulyakov <br>
                        <em>ECCV</em>, 2022
                    <div class="paper">
                        <a href="https://snap-research.github.io/R2L/">Project</a> /
                        <a href="https://github.com/snap-research/R2L">code</a> /
                        <a href="https://arxiv.org/abs/2203.17261">arXiv</a> /
                        <a href="https://arxiv.org/pdf/2203.17261.pdf">PDF</a>

                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/multimodal_3d.jpeg" alt="game" width="190" height="118"
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://people.cs.umass.edu/~zezhoucheng/edit3d/"><b>Cross-Modal 3D Shape Generation
                                and
                                Manipulation</b></a><br>
                        Zezhou Cheng, Menglei Chai, <strong>Jian Ren</strong>, Hsin-Ying Lee, Kyle Olszewski, Zeng
                        Huang, <br> Subhransu Maji, Sergey Tulyakov <br>
                        <em>ECCV</em>, 2022
                    <div class="paper">
                        <a href="https://people.cs.umass.edu/~zezhoucheng/edit3d/">Project</a> /
                        <a href="https://github.com/snap-research/edit3d">code</a> /
                        <a href="https://arxiv.org/abs/2207.11795">arXiv</a> /
                        <a href="https://arxiv.org/pdf/2207.11795.pdf">PDF</a>
                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/mmvid.gif" alt="game" width="190" height="153"
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://snap-research.github.io/MMVID/"><b>Show Me What and Tell Me How: Video
                                Synthesis
                                via Multimodal Conditioning</b></a><br>
                        Ligong Han, <strong>Jian Ren</strong>, Hsin-Ying Lee, Francesco Barbieri, Kyle Olszewski,
                        <br>
                        Shervin Minaee, Dimitris Metaxas, Sergey Tulyakov<br>
                        <em>CVPR</em>, 2022
                    <div class="paper">
                        <a href="https://snap-research.github.io/MMVID/">Project</a> /
                        <a href="https://github.com/snap-research/MMVID">code</a> /
                        <a href="https://arxiv.org/abs/2203.02573">arXiv</a> /
                        <a href="https://arxiv.org/pdf/2203.02573.pdf">PDF</a>

                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>
        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/robust_t.png" alt="game" width="190" height="96"
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://arxiv.org/abs/2204.05454/"><b>Are Multimodal Transformers Robust to
                                Missing Modality?
                            </b></a><br>

                        Mengmeng Ma, <strong>Jian Ren</strong>, Long Zhao, Davide Testuggine,
                        Xi Peng <br>
                        <em>CVPR</em>, 2022
                    <div class="paper">
                        <a href="https://arxiv.org/pdf/2204.05454.pdf">PDF</a> /
                        <a href="https://arxiv.org/abs/2204.05454">arXiv</a>

                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/inout.png" alt="game" width="190" height="107"
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://yccyenchicheng.github.io/InOut/"><b>In&Out: Diverse Image Outpainting via
                                GAN
                                Inversion
                            </b></a><br>
                        Yen-Chi Cheng, Chieh Hubert Lin, Hsin-Ying Lee, <strong>Jian Ren</strong>, Sergey
                        Tulyakov,<br>
                        Ming-Hsuan Yang<br>
                        <em>CVPR</em>, 2022
                    <div class="paper">
                        <a href="https://yccyenchicheng.github.io/InOut/">Project</a> /
                        <a href="https://arxiv.org/abs/2104.00675">arXiv</a> /
                        <a href="https://github.com/yccyenchicheng/InOut">code</a> /
                        <a href="https://arxiv.org/pdf/2104.00675.pdf">PDF</a>

                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/f8net.png" alt="game" width="180" height="180"
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://openreview.net/forum?id=_CfpJazzXT2"><b>F8Net: Fixed-Point 8-bit Only
                                Multiplication for Network
                                Quantization</b></a><br>
                        Qing Jin, <strong>Jian Ren</strong>, Richard Zhuang, Sumant Hanumante, Zhengang Li,
                        Zhiyu Chen, <br> Yanzhi Wang, Kaiyuan Yang, Sergey Tulyakov<br>
                        <em>ICLR</em>, 2022 <strong>(Oral)</strong><br>
                    <div class="paper">
                        <a href="https://openreview.net/forum?id=_CfpJazzXT2">OpenReview</a> /
                        <a href="https://openreview.net/pdf?id=_CfpJazzXT2">PDF</a> /
                        <a href="https://arxiv.org/abs/2202.05239v1">arXiv</a> /
                        <a href="https://github.com/snap-research/f8net">code</a>

                    </div>
                </td>
                </td>
            </table>
        </div>
        <!-- <hr> -->
    </div>
    <br>

    <div class="container">
        <h2> 2021 </h2>
        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/lottery.png" alt="game" width="180" height="161"
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://arxiv.org/abs/2102.11068"><b>Lottery Ticket Implies Accuracy Degradation, Is
                                It
                                a Desirable
                                Phenomenon?</b></a><br>
                        Ning Liu, Geng Yuan, Zhengping Che, Xuan Shen, Xiaolong Ma, Qing Jin,
                        <strong>Jian Ren</strong>, <br>Jian Tang, Sijia Liu, Yanzhi Wang<br>
                        <em>ICML</em>, 2021 <br>
                    <div class="paper">
                        <a href="https://arxiv.org/abs/2102.11068">arXiv</a> /
                        <a href="https://arxiv.org/pdf/2102.11068.pdf">PDF</a>

                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>


        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/Youtube_Dancing_driving.gif" alt="game" width="180"
                        height="146" style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://arxiv.org/abs/2106.07771"><b>Flow Guided Transformable Bottleneck Networks
                                for
                                Motion Retargeting</b></a><br>
                        <strong>Jian Ren</strong>, Menglei Chai, Oliver Woodford, Kyle Olszewski,
                        Sergey Tulyakov<br>
                        <em>CVPR</em>, 2021 <br>

                    <div class="paper">
                        <a href="https://arxiv.org/abs/2106.07771">arXiv</a> /
                        <a href="https://arxiv.org/pdf/2106.07771.pdf">PDF</a>
                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/human-motion-transfer.gif" alt="game" width="190"
                        height="129" style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://snap-research.github.io/articulated-animation/"><b>Motion Representations
                                for
                                Articulated
                                Animation</b></a><br>
                        Aliaksandr Siarohin, Oliver Woodford, <strong>Jian Ren</strong>, Menglei Chai,
                        Sergey Tulyakov<br>
                        <em>CVPR</em>, 2021 <br>

                    <div class="paper">
                        <a href="https://snap-research.github.io/articulated-animation/">Project</a> /
                        <a href=" https://arxiv.org/abs/2104.11280">arXiv</a> /
                        <a href="https://github.com/snap-research/articulated-animation">code</a> /
                        <a href="https://arxiv.org/pdf/2104.11280.pdf">PDF</a> /
                        <a href="https://www.youtube.com/watch?v=gpBYN8t8_yY">YouTube</a>
                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/compression_gan.png" alt="game" width="190"
                        height="147" style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://dejqk.github.io/GAN_CAT"><b>Teachers Do More Than Teach: Compressing
                                Image-to-Image Models</b></a><br>
                        Qing Jin, <strong>Jian Ren</strong>, Oliver J. Woodford, Jiazhuo Wang, Geng Yuan,
                        Yanzhi Wang, Sergey Tulyakov<br>
                        <em>CVPR</em>, 2021 <br>

                    <div class="paper">
                        <a href="https://dejqk.github.io/GAN_CAT/">Project</a> /
                        <a href=" https://arxiv.org/abs/2103.03467">arXiv</a> /
                        <a href="https://github.com/snap-research/CAT/">code</a> /
                        <a href="https://arxiv.org/pdf/2103.03467.pdf">PDF</a> /
                        <a href="https://lensstudio.snapchat.com/guides/machine-learning/lens-templates/">Tutorial (
                            Image To Image Translation)</a> /
                        <a href="https://ar.snap.com/image-to-image-translation">Blog</a>
                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/mocogan-hd.gif" alt="game" width="190" height="107"
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://bluer555.github.io/MoCoGAN-HD"><b>A Good Image Generator Is What You Need
                                for
                                High-Resolution Video Synthesis</b></a><br>
                        Yu Tian, <strong>Jian Ren</strong>, Menglei Chai, Kyle Olszewski, Xi Peng, Dimitris
                        Metaxas,<br> Sergey Tulyakov<br>
                        <em>ICLR</em>, 2021 <strong>(Spotlight)</strong><br>

                    <div class="paper">
                        <a href="https://bluer555.github.io/MoCoGAN-HD/">Project</a> /
                        <a href="https://openreview.net/forum?id=6puCSjH3hwA">OpenReview</a> /
                        <a href="https://arxiv.org/abs/2104.15069">arXiv</a> /
                        <a href="https://github.com/snap-research/MoCoGAN-HD/">code</a> /
                        <a href="https://arxiv.org/pdf/2104.15069.pdf">PDF</a> /
                        <a href="https://papertalk.org/papertalks/29015">Talk & Slides</a>
                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/smil.png" alt="game" width="190" height="110"
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://arxiv.org/abs/2103.05677"><b>SMIL: Multimodal learning with severely missing
                                modality</b></a><br>
                        Mengmeng Ma, <strong>Jian Ren</strong>, Long Zhao, Sergey Tulyakov, Cathy Wu, Xi
                        Peng<br>
                        <em>AAAI</em>, 2021<br>

                    <div class="paper">
                        <a href="https://arxiv.org/abs/2103.05677">arXiv</a> /
                        <a href="https://github.com/mengmenm/SMIL">code</a> /
                        <a href="https://www.youtube.com/watch?v=Nz3IlSIhf6I">YouTube</a> /
                        <a href="https://arxiv.org/pdf/2103.05677.pdf">PDF</a>
                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <!-- <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/hair-rendering.gif" alt="game" width="190" height="103"
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://arxiv.org/abs/2004.13297"><b>Neural Hair Rendering</b></a><br>
                        Menglei Chai, <strong>Jian Ren</strong>, Sergey Tulyakov<br>
                        <em>ECCV</em>, 2020 <br>

                    <div class="paper" id="interactnet">
                        <a href="https://arxiv.org/abs/2004.13297">arXiv</a> /
                        <a href="https://arxiv.org/pdf/2004.13297.pdf">PDF</a>
                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr> -->
        <p style="text-align:right;"> <a href="https://gkioxari.github.io/">Webpage Credits</a></p>
    </div>
    <br>




    <script xml:space="preserve" language="JavaScript">
        hideallbibs();
    </script>


</body>

</html>