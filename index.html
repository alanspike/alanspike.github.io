<!DOCTYPE html>
<html lang="en">

<head>
    <link rel="shortcut icon" type="image/x-icon" href="images/head.png" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Jian Ren's Personal Academic Website">

    <title>Jian Ren</title>

    <!-- Preload critical resources -->
    <link rel="preload" href="css/style.css" as="style">
    <link rel="preload" href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;700&display=swap" as="style">
    
    <!-- Modern CSS framework - minimal version -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" integrity="sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM" crossorigin="anonymous">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;700&display=swap" rel="stylesheet">
    <link href="css/style.css" rel="stylesheet" type="text/css" />
    
    <!-- Custom styles for modernization -->
    <style>
        :root {
            --primary-color: #1772d0;
            --accent-color: #f09228;
            --bg-color: #f8f9fa;
            --text-color: #333;
            --container-bg: #ffffff;
        }
        
        body {
            font-family: 'Inter', sans-serif;
            background-color: var(--bg-color);
            color: var(--text-color);
            line-height: 1.6;
            padding-top: 20px;
            -webkit-text-size-adjust: 100%; /* Prevent font scaling in landscape */
        }
        
        .container {
            max-width: 1000px;
            width: 95%; /* Allow containers to adjust based on screen width */
            background-color: var(--container-bg);
            border-radius: 12px;
            box-shadow: 0 4px 12px rgba(0,0,0,0.05);
            padding: 30px;
            margin-bottom: 25px;
            transition: transform 0.3s ease, box-shadow 0.3s ease;
        }
        
        .container:hover {
            transform: translateY(-3px);
            box-shadow: 0 6px 18px rgba(0,0,0,0.08);
        }
        
        h2 {
            font-weight: 600;
            color: var(--primary-color);
            margin-bottom: 20px;
            position: relative;
            padding-bottom: 8px;
        }
        
        h2:after {
            content: '';
            position: absolute;
            width: 60px;
            height: 3px;
            background-color: var(--accent-color);
            bottom: 0;
            left: 0;
        }
        
        a {
            color: var(--primary-color);
            text-decoration: none;
            transition: color 0.2s ease;
        }
        
        a:hover {
            color: var(--accent-color);
            text-decoration: none;
        }
        
        .publication {
            margin-bottom: 25px;
        }
        
        .publication img {
            border-radius: 8px;
            transition: transform 0.3s ease;
            max-width: 100%;
            height: auto;
        }
        
        .publication img:hover {
            transform: scale(1.03);
        }
        
        .paper a {
            padding: 4px 8px;
            background-color: #f0f0f0;
            border-radius: 4px;
            margin-right: 5px;
            font-size: 0.9em;
            display: inline-block;
            margin-top: 5px;
            transition: background-color 0.2s ease;
        }
        
        .paper a:hover {
            background-color: #e0e0e0;
        }
        
        .social-links {
            display: flex;
            justify-content: center;
            gap: 15px;
            margin-bottom: 20px;
        }
        
        .social-links a {
            transition: transform 0.3s ease;
        }
        
        .social-links a:hover {
            transform: translateY(-3px);
        }
        
        hr {
            margin: 25px 0;
            background-color: rgba(0,0,0,0.1);
        }
        
        /* Media queries for responsiveness */
        @media (max-width: 768px) {
            .container {
                padding: 20px;
                margin-bottom: 15px;
            }
            
            .publication td {
                display: block;
                width: 100% !important;
                text-align: center;
            }
            
            .publication td:first-child {
                margin-bottom: 15px;
            }
            
            .publication td:last-child {
                text-align: left;
            }
        }
        
        /* Fix for media coverage section alignment */
        .media-coverage-section h3,
        .media-coverage-section p {
            text-align: left !important;
        }
        
        /* iPhone and mobile specific styles */
        @media only screen and (max-width: 480px) {
            body {
                padding-top: 10px;
            }
            
            .container {
                padding: 15px;
                margin-bottom: 15px;
                border-radius: 8px;
            }
            
            h1 {
                font-size: 24px;
            }
            
            h2 {
                font-size: 20px;
            }
            
            h3 {
                font-size: 18px;
            }
            
            .publication table {
                display: block;
                width: 100%;
            }
            
            .publication tr {
                display: flex;
                flex-direction: column;
            }
            
            .publication td {
                display: block;
                width: 100% !important;
                padding: 5px 0;
            }
            
            .publication td:first-child {
                text-align: center;
                margin-bottom: 10px;
            }
            
            .publication img {
                max-width: 80%;
                height: auto;
            }
            
            .paper a {
                display: inline-block;
                margin: 5px 3px;
                padding: 8px 12px; /* Bigger tap targets */
            }
            
            .social-links {
                gap: 20px; /* More space between social icons for easier tapping */
            }
            
            .social-links a img {
                height: 28px; /* Slightly larger icons for better tappability */
            }
        }
        
        /* Fix tap target sizes for better mobile usability */
        @media (hover: none) {
            a, button {
                min-height: 44px;
                min-width: 44px;
            }
            
            .paper a {
                padding: 8px 12px;
            }
        }
        
        /* Prevent horizontal scrolling on mobile */
        html, body {
            overflow-x: hidden;
            width: 100%;
        }
    </style>

    <script type="text/javascript">
        var _gaq = _gaq || [];
        _gaq.push(['_setAccount', 'UA-40545479-1']);
        _gaq.push(['_trackPageview']);

        (function () {
            var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
            ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
            var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
        })();
    </script>
</head>

<body>
    <div class="container">
        <div class="row justify-content-center">
            <div class="col-md-10">
                <h1 class="text-center mb-4">Jian Ren</h1>
                
                <div class="social-links">
                    <a href="mailto:renjianustc@gmail.com" target="_blank"><img src="icons/email.png" height="24" alt="Email"></a>
                    <a href="https://www.linkedin.com/in/renjian0905" target="_blank"><img src="icons/linkedin.png" height="24" alt="LinkedIn"></a>
                    <a href="https://scholar.google.com/citations?user=vDALiU4AAAAJ&hl=en&authuser=1" target="_blank"><img src="icons/google_scholar.png" height="24" alt="Google Scholar"></a>
                    <a href="https://github.com/alanspike/" target="_blank"><img src="icons/github.png" height="24" alt="GitHub"></a>
                    <a href="https://twitter.com/JianRen_" target="_blank"><img src="icons/twitter.png" height="24" alt="Twitter"></a>
                </div>
                
                <div class="text-center mb-4">
                    <iframe src="https://giphy.com/embed/Mah9dFWo1WZX0WM62Q" width="200" frameBorder="0" class="giphy-embed" allowFullScreen loading="lazy"></iframe>
                </div>
            </div>
        </div>
    </div>

    <div class="container media-coverage-section">
        <h3 class="mb-3">Selected Media Coverage</h3>
        <div class="row">
            <div class="col-12">
                <p><strong>Image Generation Models:</strong> TechCrunch (<a href="https://techcrunch.com/2025/02/04/snap-unveils-ai-text-to-image-model-for-mobile-devices/">1</a>, <a href="https://techcrunch.com/2024/06/18/snap-previews-its-real-time-image-model-that-can-generate-ar-experiences/">2</a>), <a href="https://www.theverge.com/2024/6/19/24181965/snapchat-ai-prompt-custom-lens">The Verge</a>, Snap Newsroom (<a href="https://newsroom.snap.com/ai-text-to-image-model-for-mobile-devices">1</a>, <a href="https://newsroom.snap.com/new-ar-experiences-powered-by-gen-ai">2</a>)</p>
                <p><strong>3D Generation and Rendering:</strong> <a href="https://www.youtube.com/watch?v=v5tdnx4yz80">YouTube</a>, <a href="https://www.reuters.com/technology/snap-launches-ai-tools-advanced-augmented-reality-2024-06-18/">Reuters</a></p>
                <p><strong>Video Generation Models:</strong> TechCrunch (<a href="https://techcrunch.com/2024/09/17/snap-is-introducing-an-ai-video-generation-tool-for-creators/">1</a>, <a href="https://techcrunch.com/2025/03/12/snap-introduces-ai-video-lenses-powered-by-its-in-house-generative-model/">2</a>)</p>
            </div>
        </div>
    </div>

    <div class="container">
        <h2>2025</h2>

        <div class="publication">
            <table width="100%" border="0" cellpadding="0">
                <tr>
                    <td width="40%" valign="center"><img src="images/publication_snapgen.png" alt="SnapGen" width="180" 
                        style="border-style: none" loading="lazy"></td>
                    <td width="60%" valign="top">
                        <p><a href="https://snap-research.github.io/snapgen/"><b>
                            SnapGen: Taming High-Resolution Text-to-Image Models for Mobile Devices with Efficient Architectures and Training
                                </b></a><br>
                                Dongting Hu, Jierun Chen, Xijie Huang, Huseyin Coskun, Arpit Sahni, Aarush Gupta, Anujraaj Goyal, Dishani Lahiri, Rajesh Singh, Yerlan Idelbayev, Junli Cao, Yanyu Li, Kwang-Ting Cheng, S.-H. Gary Chan, Mingming Gong, Sergey Tulyakov, Anil Kag, Yanwu Xu,  <strong>Jian Ren</strong>
                            <br>
                            <em>CVPR</em>, 2025  <strong>(Highlight)</strong><br>
                        <div class="paper">
                            <a href="https://snap-research.github.io/snapgen/">Project</a>
                            <a href="https://arxiv.org/abs/2412.09619">arXiv</a>
                            <a href="https://arxiv.org/pdf/2412.09619">PDF</a>
                        </div>
                    </td>
                </tr>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/wonderland.gif" alt="game" width="190" 
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://snap-research.github.io/wonderland/"><b>
                        Wonderland: Navigating 3D Scenes from a Single Image
                    </b></a><br>
                            Hanwen Liang, Junli Cao, Vidit Goel, Guocheng Qian, Sergei Korolev, Demetri Terzopoulos, Konstantinos N. Plataniotis, Sergey Tulyakov, 
                            <strong>Jian Ren</strong>
                        <br>
                        <em>CVPR</em>, 2025<br>
                    <div class="paper">
                        <a href="https://snap-research.github.io/wonderland/">Project</a> /
                        <a href="https://arxiv.org/abs/2412.12091">arXiv</a> /
                        <a href="https://arxiv.org/pdf/2412.12091">PDF</a>
                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/snapgen-v-pipeline.png" alt="game" width="190" 
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://snap-research.github.io/snapgen-v/"><b>
                        SnapGen-V: Generating a Five-Second Video within Five Seconds on a Mobile Device
                            </b></a><br>
                            Yushu Wu, Zhixing Zhang, Yanyu Li, Yanwu Xu, Anil Kag, Yang Sui, Huseyin Coskun, Ke Ma, Aleksei Lebedev, Ju Hu, Dimitris Metaxas, Yanzhi Wang, Sergey Tulyakov, <strong>Jian Ren</strong>

                            
                        <br>
                        <em>CVPR</em>, 2025<br>
                    <div class="paper">
                        <a href="https://snap-research.github.io/snapgen-v/">Project</a> /
                        <a href="https://arxiv.org/abs/2412.10494">arXiv</a> /
                        <a href="https://arxiv.org/pdf/2412.10494">PDF</a>
                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>


        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/lightweight-gs.jpg" alt="game" width="190" 
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://arxiv.org/abs/2406.19434"><b>
                        Lightweight Predictive 3D Gaussian Splats
                            </b></a><br>
                            Junli Cao, Vidit Goel, Chaoyang Wang, Anil Kag, Ju Hu, Sergei Korolev, <br> Chenfanfu Jiang, Sergey Tulyakov,  <strong>Jian Ren</strong>                           
                        <br>
                        <em>ICLR</em>, 2025<br>
                    <div class="paper">
                        <a href="https://arxiv.org/abs/2406.19434">arXiv</a> /
                        <a href="https://arxiv.org/pdf/2406.19434">PDF</a>
                    </div>
                </td>
                </td>
            </table>
        </div>

    </div>
    <br>

    <div class="container">
        <h2> 2024 </h2>
        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/ascan.jpg" alt="game" width="190" 
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://snap-research.github.io/snap_image/"><b>
                        AsCAN: Asymmetric Convolution-Attention Networks for Efficient Recognition and Generation
                            </b></a><br>
                            Anil Kag, Huseyin Coskun, Jierun Chen, Junli Cao, Willi Menapace, Aliaksandr Siarohin, <br> Sergey Tulyakov, 
                            <strong>Jian Ren</strong>
                        <br>
                        <em>NeurIPs</em>, 2024<br>
                    <div class="paper">
                        <a href="https://snap-research.github.io/snap_image/">Project</a> /
                        <a href="https://arxiv.org/abs/2411.04967">arXiv</a> /
                        <a href="https://arxiv.org/pdf/2411.04967">PDF</a>
                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>
        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/sf-v-1.gif" alt="game" width="190" 
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://snap-research.github.io/SF-V/"><b>
                        SF-V: Single Forward Video Generation Model
                            </b></a><br>
                            Zhixing Zhang, Yanyu Li, Yushu Wu, Yanwu Xu, Anil Kag, Ivan Skorokhodov, Willi Menapace, Aliaksandr Siarohin, Junli Cao, Dimitris Metaxas, Sergey Tulyakov,  <strong>Jian Ren</strong>
                        <br>
                        <em>NeurIPs</em>, 2024<br>
                    <div class="paper">
                        <a href="https://snap-research.github.io/SF-V/">Project</a> /
                        <a href="https://arxiv.org/abs/2406.04324">arXiv</a> /
                        <a href="https://arxiv.org/pdf/2406.04324">PDF</a>
                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>
        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/bitsfusion.jpg" alt="game" width="190" 
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://snap-research.github.io/BitsFusion/"><b>
                        BitsFusion: 1.99 bits Weight Quantization of Diffusion Model
                            </b></a><br>
                            Yang Sui,   Yanyu Li,  Anil Kag, Yerlan Idelbayev, Junli Cao,  Ju Hu, Dhritiman Sagar, <br> Bo Yuan, Sergey Tulyakov,
                            <strong>Jian Ren</strong>
                        <br>
                        <em>NeurIPs</em>, 2024<br>
                    <div class="paper">
                        <a href="https://snap-research.github.io/BitsFusion/">Project</a> /
                        <a href="https://arxiv.org/abs/2406.04333">arXiv</a> /
                        <a href="https://arxiv.org/pdf/2406.04333">PDF</a>
                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>


        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/weight_generator.jpg" alt="game" width="190" 
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://yifanfanfanfan.github.io/denoised-weights/"><b>
                        Efficient Training with Denoised Neural Weights
                            </b></a><br>
                            Yifan Gong, Zheng Zhan, Yanyu Li, Yerlan Idelbayev, Andrey Zharkov,<br> Kfir Aberman, Sergey Tulyakov, Yanzhi Wang, 
                            <strong>Jian Ren</strong>
                        <br>
                        <em>ECCV</em>, 2024<br>
                    <div class="paper">
                        <a href="https://yifanfanfanfan.github.io/denoised-weights/">Project</a> /
                        <a href="https://arxiv.org/abs/2407.11966">arXiv</a> /
                        <a href="https://arxiv.org/pdf/2407.11966">PDF</a>
                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/e2gan.jpeg" alt="game" width="190" 
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://yifanfanfanfan.github.io/e2gan/"><b>
                        E<sup>2</sup>GAN: Efficient Training of Efficient GANs for Image-to-Image Translation
                            </b></a><br>
                            Yifan Gong, Zheng Zhan, Qing Jin, Yanyu Li, Yerlan Idelbayev, Xian Liu, <br>Andrey Zharkov, Kfir Aberman, Sergey Tulyakov, Yanzhi Wang, 
                            <strong>Jian Ren</strong>
                        <br>
                        <em>ICML</em>, 2024<br>
                    <div class="paper">
                        <a href="https://yifanfanfanfan.github.io/e2gan/">Project</a> /
                        <a href="https://arxiv.org/abs/2401.06127">arXiv</a> /
                        <a href="https://arxiv.org/pdf/2401.06127">PDF</a>
                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/textcraftor.png" alt="game" width="190" 
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://arxiv.org/abs/2403.18978"><b>
                        TextCraftor: Your Text Encoder Can be Image Quality Controller
                            </b></a><br>
                            Yanyu Li, Xian Liu, Anil Kag, Ju Hu, Yerlan Idelbayev, Dhritiman Sagar, Yanzhi Wang, <br>Sergey Tulyakov, <strong>Jian Ren</strong>
                        <br>
                        <em>CVPR</em>, 2024<br>
                    <div class="paper">
                        <a href="https://arxiv.org/abs/2403.18978">arXiv</a> /
                        <a href="https://arxiv.org/pdf/2403.18978">PDF</a>
                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/snapvideo.gif" alt="game" width="190" 
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://snap-research.github.io/snapvideo/"><b>
                        Snap Video: Scaled Spatiotemporal Transformers for Text-to-Video Synthesis
                            </b></a><br>
                            Willi Menapace, Aliaksandr Siarohin, Ivan Skorokhodov, Ekaterina Deyneka, <br>Tsai-Shien Chen, Anil Kag, Yuwei Fang, Aleksei Stoliar, Elisa Ricci, <strong>Jian Ren</strong>, Sergey Tulyakov
                        <br>
                        <em>CVPR</em>, 2024 <strong>(Highlight)</strong><br>
                    <div class="paper">
                        <a href="https://snap-research.github.io/snapvideo/">Project</a> /
                        <a href="https://arxiv.org/abs/2402.14797">arXiv</a> /
                        <a href="https://arxiv.org/pdf/2402.14797">PDF</a>
                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/panda.png" alt="game" width="190" 
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://snap-research.github.io/Panda-70M/"><b>
                        Panda-70M: Captioning 70M Videos with Multiple Cross-Modality Teachers
                            </b></a><br>
                            Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, <br>Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, 
                            <strong>Jian Ren</strong>, Ming-Hsuan Yang, Sergey Tulyakov
                        <br>
                        <em>CVPR</em>, 2024<br>
                    <div class="paper">
                        <a href="https://snap-research.github.io/Panda-70M/">Project</a> /
                        <a href="https://github.com/snap-research/Panda-70M">code</a> /
                        <a href="https://arxiv.org/abs/2402.19479">arXiv</a> /
                        <a href="https://arxiv.org/pdf/2402.19479">PDF</a>
                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/spad.gif" alt="game" width="190" 
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://yashkant.github.io/spad/"><b>
                        SPAD: Spatially Aware Multiview Diffusers
                            </b></a><br>
                            Yash Kant, Ziyi Wu, Michael Vasilkovsky, Guocheng Qian, <strong>Jian Ren</strong>, Riza Alp Guler, <br> 
                            Bernard Ghanem, Sergey Tulyakov, Igor Gilitschenski, Aliaksandr Siarohin
                        <br>
                        <em>CVPR</em>, 2024<br>
                    <div class="paper">
                        <a href="https://yashkant.github.io/spad/">Project</a> /
                        <a href="https://github.com/yashkant/spad">code</a> /
                        <a href="https://arxiv.org/abs/2402.05235">arXiv</a> /
                        <a href="https://arxiv.org/pdf/2402.05235">PDF</a>
                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/hyperhuman.gif" alt="game" width="190" 
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://snap-research.github.io/HyperHuman/"><b>
                        HyperHuman: Hyper-Realistic Human Generation with Latent Structural Diffusion
                            </b></a><br>
                        Xian Liu, <strong>Jian Ren</strong>, Aliaksandr Siarohin, Ivan Skorokhodov, Yanyu Li,<br> 
                        Dahua Lin, Xihui Liu, Ziwei Liu, Sergey Tulyakov
                        <br>
                        <em>ICLR</em>, 2024<br>
                    <div class="paper">
                        <a href="https://snap-research.github.io/HyperHuman/">Project</a> /
                        <a href="https://openreview.net/forum?id=duyA42HlCK/">OpenReview</a> /
                        <a href="https://arxiv.org/abs/2310.08579">arXiv</a> /
                        <a href="https://arxiv.org/pdf/2310.08579.pdf">PDF</a>
                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/magic123.gif" alt="game" width="190" 
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://guochengqian.github.io/project/magic123/"><b>
                        Magic123: One Image to High-Quality 3D Object Generation Using Both 2D and 3D Diffusion Priors
                            </b></a><br>
                            Guocheng Qian, Jinjie Mai, Abdullah Hamdi, <strong>Jian Ren</strong>, Aliaksandr Siarohin, Bing Li, <br>
                            Hsin-Ying Lee, Ivan Skorokhodov, Peter Wonka, Sergey Tulyakov, Bernard Ghanem<br>
                        <em>ICLR</em>, 2024<br>
                    <div class="paper">
                        <a href="https://guochengqian.github.io/project/magic123/">Project</a> /
                        <a href="https://openreview.net/forum?id=0jHkUDyEO9/">OpenReview</a> /
                        <a href="https://arxiv.org/abs/2306.17843">arXiv</a> /
                        <a href="https://arxiv.org/pdf/2306.17843.pdf">PDF</a> /
                        <a href="https://github.com/guochengqian/Magic123">code</a>
                    </div>
                </td>
                </td>
            </table>
        </div>
        <!-- <hr> -->
    </div>
    <br>

    <div class="container">
        <h2> 2023 </h2>
        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/snapinfocusfast_demo_short.gif" alt="game" width="110" 
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://snap-research.github.io/SnapFusion/"><b>
                        SnapFusion: Text-to-Image Diffusion Model on Mobile Devices within Two Seconds
                            </b></a><br>
                        Yanyu Li, Huan Wang, Qing Jin, Ju Hu, Pavlo Chemerys, Yun Fu, Yanzhi Wang, <br>Sergey Tulyakov, <strong>Jian Ren</strong><br>
                        <em>NeurIPs</em>, 2023<br>
                    <div class="paper">
                        <a href="https://snap-research.github.io/SnapFusion/">Project</a> /
                        <a href="https://arxiv.org/abs/2306.00980">arXiv</a> /
                        <a href="https://arxiv.org/pdf/2306.00980.pdf">PDF</a>
                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/lightspeed.png" alt="game" width="190" 
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://lightspeed-r2l.github.io"><b>LightSpeed: Light and Fast Neural Light Fields on Mobile Devices
                            </b></a><br>
                            Aarush Gupta, Junli Cao, Chaoyang Wang, Ju Hu, Sergey Tulyakov, <br>
                        <strong>Jian Ren</strong>, László A Jeni <br>
                        <em>NeurIPs</em>, 2023
                    <div class="paper">
                        <a href="https://lightspeed-r2l.github.io/">Project</a> /
                        <a href="https://arxiv.org/abs/2310.16832">arXiv</a> /
                        <a href="https://arxiv.org/pdf/2310.16832.pdf">PDF</a>
                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/invs_video.gif" alt="game" width="190" 
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://yashkant.github.io/invs/"><b>iNVS: Repurposing Diffusion Inpainters for Novel View Synthesis
                            </b></a><br>
                            Yash Kant, Aliaksandr Siarohin, Michael Vasilkovsky, Riza Alp Guler, <br>
                            <strong>Jian Ren</strong>, Sergey Tulyakov, Igor Gilitschenski<br>
                    <em>SIGGRAPH Asia</em>, 2023
                    <div class="paper">
                        <a href="https://yashkant.github.io/invs/">Project</a> /
                        <a href="https://arxiv.org/abs/2310.16167">arXiv</a> /
                        <a href="https://arxiv.org/pdf/2310.16167.pdf">PDF</a> 
                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/eformerv2.png" alt="game" width="190" height="146"
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://github.com/snap-research/EfficientFormer/"><b>Rethinking Vision Transformers for
                                MobileNet Size and Speed
                            </b></a><br>
                        Yanyu Li, Ju Hu, Yang Wen, Georgios Evangelidis, Kamyar Salahi, Yanzhi
                        Wang, <br> Sergey Tulyakov,
                        <strong>Jian Ren</strong> <br>
                        <em>ICCV</em>, 2023
                    <div class="paper">
                        <a href="https://github.com/snap-research/EfficientFormer">code</a> /
                        <a href="https://arxiv.org/abs/2212.08059">arXiv</a> /
                        <a href="https://arxiv.org/pdf/2212.08059.pdf">PDF</a>

                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/comcat.jpg" alt="game" width="190" height="92"
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://arxiv.org/abs/2305.17235"><b>COMCAT: Towards Efficient
                                Compression and Customization of
                                Attention-Based Vision Models
                            </b></a><br>
                        Jinqi Xiao, Miao Yin, Yu Gong, Xiao Zang, <strong>Jian Ren</strong>, Bo Yuan
                        <br>
                        <em>ICML</em>, 2023
                    <div class="paper">
                        <a href="https://github.com/jinqixiao/ComCAT">code</a> /
                        <a href="https://arxiv.org/abs/2305.17235">arXiv</a> /
                        <a href="https://arxiv.org/pdf/2305.17235.pdf">PDF</a>

                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/mobile-r2l.gif" alt="game" width="190" height="101"
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://snap-research.github.io/MobileR2L/"><b>
                                Real-Time Neural Light Field on Mobile Devices
                            </b></a><br>
                        Junli Cao, Huan Wang, Pavlo Chemerys, Vladislav Shakhrai, Ju Hu, Yun Fu, <br>Denys Makoviichuk,
                        Sergey
                        Tulyakov, <strong>Jian Ren</strong>
                        <br>
                        <em>CVPR</em>, 2023<br>
                    <div class="paper">
                        <a href="https://snap-research.github.io/MobileR2L/">Project</a> /
                        <a href="https://arxiv.org/abs/2212.08057">arXiv</a> /
                        <a href="https://github.com/snap-research/MobileR2L">code</a> /
                        <a href="https://arxiv.org/pdf/2212.08057.pdf">PDF</a>
                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/SINE.png" alt="game" width="190" height="170"
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://zhang-zx.github.io/SINE/"><b>
                                SINE: SINgle Image Editing with Text-to-Image Diffusion Models
                            </b></a><br>
                        Zhixing Zhang, Ligong Han, Arnab Ghosh, Dimitris N. Metaxas, <strong>Jian Ren</strong>
                        <br>
                        <em>CVPR</em>, 2023<br>
                    <div class="paper">
                        <a href="https://zhang-zx.github.io/SINE/">Project</a> /
                        <a href="https://arxiv.org/abs/2212.04489">arXiv</a> /
                        <a href="https://github.com/zhang-zx/SINE">code</a> /
                        <a href="https://arxiv.org/pdf/2212.04489.pdf">PDF</a>
                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>


        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/uva.gif" alt="game" width="190" height="160"
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://snap-research.github.io/unsupervised-volumetric-animation/"><b>Unsupervised
                                Volumetric
                                Animation

                            </b></a><br>
                        Aliaksandr Siarohin, Willi Menapace, Ivan Skorokhodov, <strong>Jian Ren</strong>, Hsin-Ying Lee,
                        <br> Menglei Chai, Kyle Olszewski, Sergey Tulyakov
                        <br>
                        <em>CVPR</em>, 2023<br>
                    <div class="paper">
                        <a href="https://snap-research.github.io/unsupervised-volumetric-animation/">Project</a> /
                        <a href="https://arxiv.org/abs/2301.11326">arXiv</a> /
                        <a href="https://github.com/snap-research/unsupervised-volumetric-animation">code</a> /
                        <a href="https://arxiv.org/pdf/2301.11326.pdf">PDF</a>
                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/make-a-story.png" alt="game" width="190" height="90"
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://yashkant.github.io/invertible-neural-skinning/"><b>Make-A-Story: Visual Memory
                                Conditioned Consistent Story Generation
                            </b></a><br>
                        Tanzila Rahman, Hsin-Ying Lee, <strong>Jian Ren</strong>, Sergey Tulyakov, Shweta Mahajan,<br>
                        Leonid Sigal
                        <br>
                        <em>CVPR</em>, 2023<br>
                    <div class="paper">
                        <a href="https://arxiv.org/abs/2211.13319">arXiv</a> /
                        <a href="https://arxiv.org/pdf/2211.13319.pdf">PDF</a>
                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/ins.gif" alt="game" width="190" height="107"
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://yashkant.github.io/invertible-neural-skinning/"><b>Invertible Neural Skinning
                            </b></a><br>
                        Yash Kant, Aliaksandr Siarohin, Riza Alp Guler, Menglei Chai, <strong>Jian Ren</strong>,
                        <br>Sergey
                        Tulyakov, Igor
                        Gilitschenski
                        <br>
                        <em>CVPR</em>, 2023<br>
                    <div class="paper">
                        <a href="https://yashkant.github.io/invertible-neural-skinning">Project</a> /
                        <a href="https://arxiv.org/abs/2302.09227">arXiv</a> /
                        <a href="https://github.com/yashkant/invertible-neural-skinning">code</a> /
                        <a href="https://www.youtube.com/watch?v=L7MrPzhqPWQ">video</a> /
                        <a href="https://arxiv.org/pdf/2302.09227.pdf">PDF</a>
                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/imagenet.gif" alt="game" width="190" height="190"
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://snap-research.github.io/3dgp/"><b>3D Generation on ImageNet
                            </b></a><br>
                        Ivan Skorokhodov, Aliaksandr Siarohin, Yinghao Xu, <strong>Jian Ren</strong>, Hsin-Ying Lee,<br>
                        Peter Wonka, Sergey Tulyakov
                        <br>
                        <em>ICLR</em>, 2023 <strong>(Oral)</strong><br>
                    <div class="paper">
                        <a href="https://snap-research.github.io/3dgp/">Project</a> /
                        <a href="https://openreview.net/forum?id=U2WjB9xxZ9q">OpenReview</a> /
                        <a href="https://arxiv.org/abs/2303.01416">arXiv</a> /
                        <a href="https://github.com/snap-research/3dgp">code</a> /
                        <a href="https://arxiv.org/pdf/2303.01416.pdf">PDF</a>
                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/cdcd.png" alt="game" width="190" height="153"
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://l-yezhu.github.io/CDCD/"><b>Discrete Contrastive Diffusion for
                                Cross-Modal and Conditional Generation
                            </b></a><br>
                        Ye Zhu, Yu Wu, Kyle Olszewski, <strong>Jian Ren</strong>, Sergey Tulyakov, Yan Yan <br>
                        <em>ICLR</em>, 2023
                    <div class="paper">
                        <a href="https://l-yezhu.github.io/CDCD/">Project</a> /
                        <a href="https://openreview.net/forum?id=1-MBdJssZ-S">OpenReview</a> /
                        <a href="https://arxiv.org/abs/2206.07771">arXiv</a> /
                        <a href="https://github.com/L-YeZhu/CDCD/">code</a> /
                        <a href="https://arxiv.org/pdf/2206.07771.pdf">PDF</a>
                    </div>
                </td>
                </td>
            </table>
        </div>
        <!-- <hr> -->


    </div>
    <br>

    <div class="container">
        <h2> 2022 </h2>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/efficientformer.png" alt="game" width="190"
                        height="148" style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://github.com/snap-research/EfficientFormer/"><b>EfficientFormer: Vision
                                Transformers at
                                MobileNet Speed
                            </b></a><br>
                        Yanyu Li, Geng Yuan, Yang Wen, Ju Hu, Georgios Evangelidis, Sergey Tulyakov, <br> Yanzhi
                        Wang,
                        <strong>Jian Ren</strong> <br>
                        <em>NeurIPs</em>, 2022
                    <div class="paper">
                        <a href="https://github.com/snap-research/EfficientFormer">code</a> /
                        <a href="https://arxiv.org/abs/2206.01191">arXiv</a> /
                        <a href="https://arxiv.org/pdf/2206.01191.pdf">PDF</a>

                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/freeze_train.png" alt="game" width="190" height="109"
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://github.com/snap-research/SpFDE"><b>Layer Freezing & Data Sieving: Missing
                                Pieces
                                of a
                                Generic Framework for Sparse Training
                            </b></a><br>
                        Geng Yuan, Yanyu Li, Sheng Li, Zhenglun Kong, Sergey Tulyakov, Xulong Tang, <br> Yanzhi
                        Wang,
                        <strong>Jian Ren</strong> <br>
                        <em>NeurIPs</em>, 2022
                    <div class="paper">
                        <a href="https://github.com/snap-research/SpFDE">code</a> /
                        <a href="https://arxiv.org/abs/2209.11204">arXiv</a> /
                        <a href="https://arxiv.org/pdf/2209.11204.pdf">PDF</a>

                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/r2l.gif" alt="game" width="190" height="95"
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://snap-research.github.io/R2L/"><b>R2L: Distilling Neural Radiance Field to
                                Neural Light Field for Efficient Novel View Synthesis</b></a><br>
                        Huan Wang, <strong>Jian Ren</strong>, Zeng Huang, Kyle Olszewski,
                        Menglei Chai, Yun Fu, Sergey Tulyakov <br>
                        <em>ECCV</em>, 2022
                    <div class="paper">
                        <a href="https://snap-research.github.io/R2L/">Project</a> /
                        <a href="https://github.com/snap-research/R2L">code</a> /
                        <a href="https://arxiv.org/abs/2203.17261">arXiv</a> /
                        <a href="https://arxiv.org/pdf/2203.17261.pdf">PDF</a>

                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/multimodal_3d.jpeg" alt="game" width="190" height="118"
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://people.cs.umass.edu/~zezhoucheng/edit3d/"><b>Cross-Modal 3D Shape Generation
                                and
                                Manipulation</b></a><br>
                        Zezhou Cheng, Menglei Chai, <strong>Jian Ren</strong>, Hsin-Ying Lee, Kyle Olszewski, Zeng
                        Huang, <br> Subhransu Maji, Sergey Tulyakov <br>
                        <em>ECCV</em>, 2022
                    <div class="paper">
                        <a href="https://people.cs.umass.edu/~zezhoucheng/edit3d/">Project</a> /
                        <a href="https://github.com/snap-research/edit3d">code</a> /
                        <a href="https://arxiv.org/abs/2207.11795">arXiv</a> /
                        <a href="https://arxiv.org/pdf/2207.11795.pdf">PDF</a>
                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/mmvid.gif" alt="game" width="190" height="153"
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://snap-research.github.io/MMVID/"><b>Show Me What and Tell Me How: Video
                                Synthesis
                                via Multimodal Conditioning</b></a><br>
                        Ligong Han, <strong>Jian Ren</strong>, Hsin-Ying Lee, Francesco Barbieri, Kyle Olszewski,
                        <br>
                        Shervin Minaee, Dimitris Metaxas, Sergey Tulyakov<br>
                        <em>CVPR</em>, 2022
                    <div class="paper">
                        <a href="https://snap-research.github.io/MMVID/">Project</a> /
                        <a href="https://github.com/snap-research/MMVID">code</a> /
                        <a href="https://arxiv.org/abs/2203.02573">arXiv</a> /
                        <a href="https://arxiv.org/pdf/2203.02573.pdf">PDF</a>

                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>
        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/robust_t.png" alt="game" width="190" height="96"
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://arxiv.org/abs/2204.05454/"><b>Are Multimodal Transformers Robust to
                                Missing Modality?
                            </b></a><br>

                        Mengmeng Ma, <strong>Jian Ren</strong>, Long Zhao, Davide Testuggine,
                        Xi Peng <br>
                        <em>CVPR</em>, 2022
                    <div class="paper">
                        <a href="https://arxiv.org/pdf/2204.05454.pdf">PDF</a> /
                        <a href="https://arxiv.org/abs/2204.05454">arXiv</a>

                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/inout.png" alt="game" width="190" height="107"
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://yccyenchicheng.github.io/InOut/"><b>In&Out: Diverse Image Outpainting via
                                GAN
                                Inversion
                            </b></a><br>
                        Yen-Chi Cheng, Chieh Hubert Lin, Hsin-Ying Lee, <strong>Jian Ren</strong>, Sergey
                        Tulyakov,<br>
                        Ming-Hsuan Yang<br>
                        <em>CVPR</em>, 2022
                    <div class="paper">
                        <a href="https://yccyenchicheng.github.io/InOut/">Project</a> /
                        <a href="https://arxiv.org/abs/2104.00675">arXiv</a> /
                        <a href="https://github.com/yccyenchicheng/InOut">code</a> /
                        <a href="https://arxiv.org/pdf/2104.00675.pdf">PDF</a>

                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/f8net.png" alt="game" width="180" height="180"
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://openreview.net/forum?id=_CfpJazzXT2"><b>F8Net: Fixed-Point 8-bit Only
                                Multiplication for Network
                                Quantization</b></a><br>
                        Qing Jin, <strong>Jian Ren</strong>, Richard Zhuang, Sumant Hanumante, Zhengang Li,
                        Zhiyu Chen, <br> Yanzhi Wang, Kaiyuan Yang, Sergey Tulyakov<br>
                        <em>ICLR</em>, 2022 <strong>(Oral)</strong><br>
                    <div class="paper">
                        <a href="https://openreview.net/forum?id=_CfpJazzXT2">OpenReview</a> /
                        <a href="https://openreview.net/pdf?id=_CfpJazzXT2">PDF</a> /
                        <a href="https://arxiv.org/abs/2202.05239v1">arXiv</a> /
                        <a href="https://github.com/snap-research/f8net">code</a>

                    </div>
                </td>
                </td>
            </table>
        </div>
        <!-- <hr> -->
    </div>
    <br>

    <div class="container">
        <h2> 2021 </h2>
        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/lottery.png" alt="game" width="180" height="161"
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://arxiv.org/abs/2102.11068"><b>Lottery Ticket Implies Accuracy Degradation, Is
                                It
                                a Desirable
                                Phenomenon?</b></a><br>
                        Ning Liu, Geng Yuan, Zhengping Che, Xuan Shen, Xiaolong Ma, Qing Jin,
                        <strong>Jian Ren</strong>, <br>Jian Tang, Sijia Liu, Yanzhi Wang<br>
                        <em>ICML</em>, 2021 <br>
                    <div class="paper">
                        <a href="https://arxiv.org/abs/2102.11068">arXiv</a> /
                        <a href="https://arxiv.org/pdf/2102.11068.pdf">PDF</a>

                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>


        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/Youtube_Dancing_driving.gif" alt="game" width="180"
                        height="146" style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://arxiv.org/abs/2106.07771"><b>Flow Guided Transformable Bottleneck Networks
                                for
                                Motion Retargeting</b></a><br>
                        <strong>Jian Ren</strong>, Menglei Chai, Oliver Woodford, Kyle Olszewski,
                        Sergey Tulyakov<br>
                        <em>CVPR</em>, 2021 <br>

                    <div class="paper">
                        <a href="https://arxiv.org/abs/2106.07771">arXiv</a> /
                        <a href="https://arxiv.org/pdf/2106.07771.pdf">PDF</a>
                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/human-motion-transfer.gif" alt="game" width="190"
                        height="129" style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://snap-research.github.io/articulated-animation/"><b>Motion Representations
                                for
                                Articulated
                                Animation</b></a><br>
                        Aliaksandr Siarohin, Oliver Woodford, <strong>Jian Ren</strong>, Menglei Chai,
                        Sergey Tulyakov<br>
                        <em>CVPR</em>, 2021 <br>

                    <div class="paper">
                        <a href="https://snap-research.github.io/articulated-animation/">Project</a> /
                        <a href=" https://arxiv.org/abs/2104.11280">arXiv</a> /
                        <a href="https://github.com/snap-research/articulated-animation">code</a> /
                        <a href="https://arxiv.org/pdf/2104.11280.pdf">PDF</a> /
                        <a href="https://www.youtube.com/watch?v=gpBYN8t8_yY">YouTube</a>
                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/compression_gan.png" alt="game" width="190"
                        height="147" style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://dejqk.github.io/GAN_CAT"><b>Teachers Do More Than Teach: Compressing
                                Image-to-Image Models</b></a><br>
                        Qing Jin, <strong>Jian Ren</strong>, Oliver J. Woodford, Jiazhuo Wang, Geng Yuan,
                        Yanzhi Wang, Sergey Tulyakov<br>
                        <em>CVPR</em>, 2021 <br>

                    <div class="paper">
                        <a href="https://dejqk.github.io/GAN_CAT/">Project</a> /
                        <a href=" https://arxiv.org/abs/2103.03467">arXiv</a> /
                        <a href="https://github.com/snap-research/CAT/">code</a> /
                        <a href="https://arxiv.org/pdf/2103.03467.pdf">PDF</a> /
                        <a href="https://lensstudio.snapchat.com/guides/machine-learning/lens-templates/">Tutorial (
                            Image To Image Translation)</a> /
                        <a href="https://ar.snap.com/image-to-image-translation">Blog</a>
                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/mocogan-hd.gif" alt="game" width="190" height="107"
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://bluer555.github.io/MoCoGAN-HD"><b>A Good Image Generator Is What You Need
                                for
                                High-Resolution Video Synthesis</b></a><br>
                        Yu Tian, <strong>Jian Ren</strong>, Menglei Chai, Kyle Olszewski, Xi Peng, Dimitris
                        Metaxas,<br> Sergey Tulyakov<br>
                        <em>ICLR</em>, 2021 <strong>(Spotlight)</strong><br>

                    <div class="paper">
                        <a href="https://bluer555.github.io/MoCoGAN-HD/">Project</a> /
                        <a href="https://openreview.net/forum?id=6puCSjH3hwA">OpenReview</a> /
                        <a href="https://arxiv.org/abs/2104.15069">arXiv</a> /
                        <a href="https://github.com/snap-research/MoCoGAN-HD/">code</a> /
                        <a href="https://arxiv.org/pdf/2104.15069.pdf">PDF</a> /
                        <a href="https://papertalk.org/papertalks/29015">Talk & Slides</a>
                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/smil.png" alt="game" width="190" height="110"
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://arxiv.org/abs/2103.05677"><b>SMIL: Multimodal learning with severely missing
                                modality</b></a><br>
                        Mengmeng Ma, <strong>Jian Ren</strong>, Long Zhao, Sergey Tulyakov, Cathy Wu, Xi
                        Peng<br>
                        <em>AAAI</em>, 2021<br>

                    <div class="paper">
                        <a href="https://arxiv.org/abs/2103.05677">arXiv</a> /
                        <a href="https://github.com/mengmenm/SMIL">code</a> /
                        <a href="https://www.youtube.com/watch?v=Nz3IlSIhf6I">YouTube</a> /
                        <a href="https://arxiv.org/pdf/2103.05677.pdf">PDF</a>
                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <!-- <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/hair-rendering.gif" alt="game" width="190" height="103"
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://arxiv.org/abs/2004.13297"><b>Neural Hair Rendering</b></a><br>
                        Menglei Chai, <strong>Jian Ren</strong>, Sergey Tulyakov<br>
                        <em>ECCV</em>, 2020 <br>

                    <div class="paper" id="interactnet">
                        <a href="https://arxiv.org/abs/2004.13297">arXiv</a> /
                        <a href="https://arxiv.org/pdf/2004.13297.pdf">PDF</a>
                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr> -->
    </div>
    <br>




    <script>
        // Add lazy loading to all images for better performance
        document.addEventListener('DOMContentLoaded', function() {
            const images = document.querySelectorAll('img:not([loading])');
            images.forEach(img => {
                img.setAttribute('loading', 'lazy');
            });
        });
        
        // This function was called in the original code, but not defined
        // Adding a placeholder definition to prevent errors
        function hideallbibs() {
            // Placeholder function
            console.log('hideallbibs function called');
        }
    </script>


</body>

</html>
