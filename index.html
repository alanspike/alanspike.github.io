<!DOCTYPE html
    PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html>

<head>
    <link rel="shortcut icon" type="image/x-icon" href="images/head.png" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />

    <title>Jian Ren</title>

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css">
    <link href='http://fonts.googleapis.com/css?family=Lato:400,700' rel='stylesheet' type='text/css'>
    <link href="css/style.css" rel="stylesheet" type="text/css" />

    <script type="text/javascript">

        var _gaq = _gaq || [];
        _gaq.push(['_setAccount', 'UA-40545479-1']);
        _gaq.push(['_trackPageview']);

        (function () {
            var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
            ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
            var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
        })();

    </script>


</head>

<body>
    <div class="container">
        <table width="900" border="0" align="center" cellpadding="20">
            <table width="90%" align="center" border="0" cellpadding="10">
                <tr>
                    <td width="100%" valign="top">
                        <p align="center">&nbsp;</p>

                        <p align="center">
                            <font size="6px">Jian Ren</font><br>
                        </p>

                        <p>
                            I am a Senior Research Scientist working in the <a
                                href="https://research.snap.com/team/category/creative-vision/"> Creative Vision</a>
                            team at Snap Inc. Before joining Snap Inc.,
                            I got a Ph.D. at <a href="https://www.rutgers.edu/">Rutgers
                                University</a> under the supervision of <a
                                href="https://gemini.cinj.rutgers.edu/rutgers-people/david-j-foran-phd/">
                                David J. Foran</a>
                            and <a href="http://manishparashar.org/"> Manish Parashar</a> in 2019,
                            and a B.S. from <a href="http://en.ustc.edu.cn/">USTC</a>
                            in 2014.

                        <p> <a href="mailto:renjianustc@gmail.com" target="_blank"><img src="icons/email.png"
                                    height="16"></a> &nbsp&nbsp
                            <a href="https://www.linkedin.com/in/renjian0905" target="_blank"><img
                                    src="icons/linkedin.png" height="16"></a> &nbsp&nbsp
                            <a href="https://scholar.google.com/citations?user=vDALiU4AAAAJ&hl=en&authuser=1"
                                target="_blank"><img src="icons/google_scholar.png" height="16"></a> &nbsp&nbsp
                            <a href="https://github.com/alanspike/" target="_blank"><img src="icons/github.png"
                                    height="16"></a>
                        </p>
                    </td>
                    <!-- <td width="50%" height="50%">
                        <div class="instructorphoto">
                            <img src="images/head.png">
                        </div>
                    </td> -->
                </tr>
            </table>
        </table>
    </div>
    <br>



    <div class="container">
        <h2> Recent Works </h2>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/efficientformer.png" alt="game" width="190"
                        height="148" style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://github.com/snap-research/EfficientFormer/"><b>EfficientFormer: Vision
                                Transformers at
                                MobileNet Speed
                            </b></a><br>
                        Yanyu Li, Geng Yuan, Yang Wen, Ju Hu, Georgios Evangelidis, Sergey Tulyakov, <br> Yanzhi Wang,
                        <strong>Jian Ren</strong> <br>
                        <em>NeurIPs</em>, 2022
                    <div class="paper">
                        <a href="https://github.com/snap-research/EfficientFormer">code</a> /
                        <a href="https://arxiv.org/abs/2206.01191">arXiv</a> /
                        <a href="https://arxiv.org/pdf/2206.01191.pdf">PDF</a>

                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/freeze_train.png" alt="game" width="190" height="109"
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://github.com/snap-research/SpFDE"><b>Layer Freezing & Data Sieving: Missing Pieces
                                of a
                                Generic Framework for Sparse Training
                            </b></a><br>
                        Geng Yuan, Yanyu Li, Sheng Li, Zhenglun Kong, Sergey Tulyakov, Xulong Tang, <br> Yanzhi Wang,
                        <strong>Jian Ren</strong> <br>
                        <em>NeurIPs</em>, 2022
                    <div class="paper">
                        <a href="https://github.com/snap-research/SpFDE">code</a> /
                        <a href="https://arxiv.org/abs/2209.11204">arXiv</a> /
                        <a href="https://arxiv.org/pdf/2209.11204.pdf">PDF</a>

                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/r2l.gif" alt="game" width="190" height="95"
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://snap-research.github.io/R2L/"><b>R2L: Distilling Neural Radiance Field to
                                Neural Light Field for Efficient Novel View Synthesis</b></a><br>
                        Huan Wang, <strong>Jian Ren</strong>, Zeng Huang, Kyle Olszewski,
                        Menglei Chai, Yun Fu, Sergey Tulyakov <br>
                        <em>ECCV</em>, 2022
                    <div class="paper">
                        <a href="https://snap-research.github.io/R2L/">Project</a> /
                        <a href="https://github.com/snap-research/R2L">code</a> /
                        <a href="https://arxiv.org/abs/2203.17261">arXiv</a> /
                        <a href="https://arxiv.org/pdf/2203.17261.pdf">PDF</a>

                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/multimodal_3d.jpeg" alt="game" width="190" height="118"
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://people.cs.umass.edu/~zezhoucheng/edit3d/"><b>Cross-Modal 3D Shape Generation and
                                Manipulation</b></a><br>
                        Zezhou Cheng, Menglei Chai, <strong>Jian Ren</strong>, Hsin-Ying Lee, Kyle Olszewski, Zeng
                        Huang, <br> Subhransu Maji, Sergey Tulyakov <br>
                        <em>ECCV</em>, 2022
                    <div class="paper">
                        <a href="https://people.cs.umass.edu/~zezhoucheng/edit3d/">Project</a> /
                        <a href="https://github.com/snap-research/edit3d">code</a> /
                        <a href="https://arxiv.org/abs/2207.11795">arXiv</a> /
                        <a href="https://arxiv.org/pdf/2207.11795.pdf">PDF</a>
                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/mmvid.gif" alt="game" width="190" height="153"
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://snap-research.github.io/MMVID/"><b>Show Me What and Tell Me How: Video Synthesis
                                via Multimodal Conditioning</b></a><br>
                        Ligong Han, <strong>Jian Ren</strong>, Hsin-Ying Lee, Francesco Barbieri, Kyle Olszewski, <br>
                        Shervin Minaee, Dimitris Metaxas, Sergey Tulyakov<br>
                        <em>CVPR</em>, 2022
                    <div class="paper">
                        <a href="https://snap-research.github.io/MMVID/">Project</a> /
                        <a href="https://github.com/snap-research/MMVID">code</a> /
                        <a href="https://arxiv.org/abs/2203.02573">arXiv</a> /
                        <a href="https://arxiv.org/pdf/2203.02573.pdf">PDF</a>

                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>
        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/robust_t.png" alt="game" width="190" height="96"
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://arxiv.org/abs/2204.05454/"><b>Are Multimodal Transformers Robust to
                                Missing Modality?
                            </b></a><br>

                        Mengmeng Ma, <strong>Jian Ren</strong>, Long Zhao, Davide Testuggine,
                        Xi Peng <br>
                        <em>CVPR</em>, 2022
                    <div class="paper">
                        <a href="https://arxiv.org/pdf/2204.05454.pdf">PDF</a> /
                        <a href="https://arxiv.org/abs/2204.05454">arXiv</a>

                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/inout.png" alt="game" width="190" height="107"
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://yccyenchicheng.github.io/InOut/"><b>In&Out: Diverse Image Outpainting via GAN
                                Inversion
                            </b></a><br>
                        Yen-Chi Cheng, Chieh Hubert Lin, Hsin-Ying Lee, <strong>Jian Ren</strong>, Sergey Tulyakov,<br>
                        Ming-Hsuan Yang<br>
                        <em>CVPR</em>, 2022
                    <div class="paper">
                        <a href="https://yccyenchicheng.github.io/InOut/">Project</a> /
                        <a href="https://arxiv.org/abs/2104.00675">arXiv</a> /
                        <a href="https://github.com/yccyenchicheng/InOut">code</a> /
                        <a href="https://arxiv.org/pdf/2104.00675.pdf">PDF</a>

                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/f8net.png" alt="game" width="180" height="180"
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://openreview.net/forum?id=_CfpJazzXT2"><b>F8Net: Fixed-Point 8-bit Only
                                Multiplication for Network
                                Quantization</b></a><br>
                        Qing Jin, <strong>Jian Ren</strong>, Richard Zhuang, Sumant Hanumante, Zhengang Li,
                        Zhiyu Chen, <br> Yanzhi Wang, Kaiyuan Yang, Sergey Tulyakov<br>
                        <em>ICLR</em>, 2022 <strong>(Oral)</strong><br>
                    <div class="paper">
                        <a href="https://openreview.net/forum?id=_CfpJazzXT2">OpenReview</a> /
                        <a href="https://openreview.net/pdf?id=_CfpJazzXT2">PDF</a> /
                        <a href="https://arxiv.org/abs/2202.05239v1">arXiv</a> /
                        <a href="https://github.com/snap-research/f8net">code</a>

                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/lottery.png" alt="game" width="180" height="161"
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://arxiv.org/abs/2102.11068"><b>Lottery Ticket Implies Accuracy Degradation, Is It
                                a Desirable
                                Phenomenon?</b></a><br>
                        Ning Liu, Geng Yuan, Zhengping Che, Xuan Shen, Xiaolong Ma, Qing Jin,
                        <strong>Jian Ren</strong>, <br>Jian Tang, Sijia Liu, Yanzhi Wang<br>
                        <em>ICML</em>, 2021 <br>
                    <div class="paper">
                        <a href="https://arxiv.org/abs/2102.11068">arXiv</a> /
                        <a href="https://arxiv.org/pdf/2102.11068.pdf">PDF</a>

                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>


        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/Youtube_Dancing_driving.gif" alt="game" width="180"
                        height="146" style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://arxiv.org/abs/2106.07771"><b>Flow Guided Transformable Bottleneck Networks for
                                Motion Retargeting</b></a><br>
                        <strong>Jian Ren</strong>, Menglei Chai, Oliver Woodford, Kyle Olszewski,
                        Sergey Tulyakov<br>
                        <em>CVPR</em>, 2021 <br>

                    <div class="paper">
                        <a href="https://arxiv.org/abs/2106.07771">arXiv</a> /
                        <a href="https://arxiv.org/pdf/2106.07771.pdf">PDF</a>
                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/human-motion-transfer.gif" alt="game" width="190"
                        height="129" style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://snap-research.github.io/articulated-animation/"><b>Motion Representations for
                                Articulated
                                Animation</b></a><br>
                        Aliaksandr Siarohin, Oliver Woodford, <strong>Jian Ren</strong>, Menglei Chai,
                        Sergey Tulyakov<br>
                        <em>CVPR</em>, 2021 <br>

                    <div class="paper">
                        <a href="https://snap-research.github.io/articulated-animation/">Project</a> /
                        <a href=" https://arxiv.org/abs/2104.11280">arXiv</a> /
                        <a href="https://github.com/snap-research/articulated-animation">code</a> /
                        <a href="https://arxiv.org/pdf/2104.11280.pdf">PDF</a> /
                        <a href="https://www.youtube.com/watch?v=gpBYN8t8_yY">YouTube</a>
                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/compression_gan.png" alt="game" width="190"
                        height="147" style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://dejqk.github.io/GAN_CAT"><b>Teachers Do More Than Teach: Compressing
                                Image-to-Image Models</b></a><br>
                        Qing Jin, <strong>Jian Ren</strong>, Oliver J. Woodford, Jiazhuo Wang, Geng Yuan,
                        Yanzhi Wang, Sergey Tulyakov<br>
                        <em>CVPR</em>, 2021 <br>

                    <div class="paper">
                        <a href="https://dejqk.github.io/GAN_CAT/">Project</a> /
                        <a href=" https://arxiv.org/abs/2103.03467">arXiv</a> /
                        <a href="https://github.com/snap-research/CAT/">code</a> /
                        <a href="https://arxiv.org/pdf/2103.03467.pdf">PDF</a> /
                        <a href="https://lensstudio.snapchat.com/guides/machine-learning/lens-templates/">Tutorial (
                            Image To Image Translation)</a> /
                        <a href="https://ar.snap.com/image-to-image-translation">Blog</a>
                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/mocogan-hd.gif" alt="game" width="190" height="107"
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://bluer555.github.io/MoCoGAN-HD"><b>A Good Image Generator Is What You Need for
                                High-Resolution Video Synthesis</b></a><br>
                        Yu Tian, <strong>Jian Ren</strong>, Menglei Chai, Kyle Olszewski, Xi Peng, Dimitris
                        Metaxas,<br> Sergey Tulyakov<br>
                        <em>ICLR</em>, 2021 <strong>(Spotlight)</strong><br>

                    <div class="paper">
                        <a href="https://bluer555.github.io/MoCoGAN-HD/">Project</a> /
                        <a href="https://openreview.net/forum?id=6puCSjH3hwA">OpenReview</a> /
                        <a href="https://arxiv.org/abs/2104.15069">arXiv</a> /
                        <a href="https://github.com/snap-research/MoCoGAN-HD/">code</a> /
                        <a href="https://arxiv.org/pdf/2104.15069.pdf">PDF</a> /
                        <a href="https://papertalk.org/papertalks/29015">Talk & Slides</a>
                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/smil.png" alt="game" width="190" height="110"
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://arxiv.org/abs/2103.05677"><b>SMIL: Multimodal learning with severely missing
                                modality</b></a><br>
                        Mengmeng Ma, <strong>Jian Ren</strong>, Long Zhao, Sergey Tulyakov, Cathy Wu, Xi
                        Peng<br>
                        <em>AAAI</em>, 2021<br>

                    <div class="paper">
                        <a href="https://arxiv.org/abs/2103.05677">arXiv</a> /
                        <a href="https://github.com/mengmenm/SMIL">code</a> /
                        <a href="https://www.youtube.com/watch?v=Nz3IlSIhf6I">YouTube</a> /
                        <a href="https://arxiv.org/pdf/2103.05677.pdf">PDF</a>
                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>

        <div class="publication">
            <table width="900" align="center" border="0" cellpadding="0">
                <td width="40%" valign="center"><img src="images/hair-rendering.gif" alt="game" width="190" height="103"
                        style="border-style: none">
                <td width="60%" valign="top">
                    <p><a href="https://arxiv.org/abs/2004.13297"><b>Neural Hair Rendering</b></a><br>
                        Menglei Chai, <strong>Jian Ren</strong>, Sergey Tulyakov<br>
                        <em>ECCV</em>, 2020 <br>

                    <div class="paper" id="interactnet">
                        <a href="https://arxiv.org/abs/2004.13297">arXiv</a> /
                        <a href="https://arxiv.org/pdf/2004.13297.pdf">PDF</a>
                    </div>
                </td>
                </td>
            </table>
        </div>
        <hr>


        <p style="text-align:right;"> <a href="https://gkioxari.github.io/">Webpage Credits</a></p>



        <script xml:space="preserve" language="JavaScript">
            hideallbibs();
        </script>


</body>

</html>